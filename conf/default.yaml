# excellent default.yaml 
# freely taken from https://github.com/lkhphuc/lightning-hydra-template/blob/master/configs/defaults.yaml
#
hydra: 
  run:
    # Configure output dir of each experiment programmatically from the arguments
    # Example "outputs/mnist/classifier/baseline/2021-03-10-141516"
    dir: lightning_logs/${data.name}/${model.name}/${experiment}/${now:%Y-%m-%d_%H%M%S}

# Global configurations shared between different modules
experiment: baseline

# Composing nested config with default
defaults:
  - data: iris.yaml  # Path to sub-config, can also omit the .yaml extension
  - model: convnet.yaml  # I add full path for easy navigation in vim (cursor in path, press gf)

  # cosa fa questo? boh
  # - override hydra/job_logging: default
  # - override hydra/hydra_logging: default

# Pytorch lightning trainer's argument
# default flags are commented to avoid clustering the hyperparameters
trainer:
  # accelerator: None
  # accumulate_grad_batches: 1
  # amp_backend: native
  # amp_level: O2
  # auto_lr_find: False
  # auto_scale_batch_size: False
  # auto_select_gpus: False
  benchmark: True
  # check_val_every_n_epoch: 1
  # checkpoint_callback: True
  # default_root_dir:
  # deterministic: False
  # fast_dev_run: False
  # flush_logs_every_n_steps: 100
  gpus:
      1
  gradient_clip_val: 1.0
  # limit_predict_batches: 1.0
  # limit_test_batches: 1.0
  # limit_train_batches: 1.0
  # limit_val_batches: 1.0
  # log_every_n_steps: 50
  # log_gpu_memory: False
  # logger: True
  max_epochs: 60
  # max_steps: None
  # min_epochs: None
  # min_steps: None
  # move_metrics_to_cpu: False
  # multiple_trainloader_mode: max_size_cycle
  # num_nodes: 1
  # num_processes: 1
  # num_sanity_val_steps: 2
  # overfit_batches: 0.0
  # plugins: None
  # precision: 16
  # prepare_data_per_node: True
  # process_position: 0
  # profiler: None
  # progress_bar_refresh_rate: None
  # reload_dataloaders_every_epoch: False
  # replace_sampler_ddp: True
  # resume_from_checkpoint: None
  # stochastic_weight_avg: False
  # sync_batchnorm: False
  terminate_on_nan: True
  # track_grad_norm: -1
  # truncated_bptt_steps: None
  # val_check_interval: 1.0
  # weights_save_path: None
  # weights_summary: top
